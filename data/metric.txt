Text simplification (TS) requires models to transform sentences into both lexicon-wise and structure-wise simpler variants. Traditional n-gram matching based evaluation metrics rely heavily on the exact token match and human-annotated simplified sentences. 

In this paper, we present BETS, the first neural network-based reference-free TS metric that leverages pretrained contextualized language representation models and large-scale paraphrasing datasets to evaluate the comparative simplicity and meaning preservation between input and output sentences directly. 

Comparative study shows that our metric correlates better than existing metrics with human judgments for the quality of both overall simplification and its key aspects, e.g., comparative simplicity and meaning preservation. The relief of reference and the introduction of controllable coefficients further improve the feasibility of applying TS to new datasets and new domains.